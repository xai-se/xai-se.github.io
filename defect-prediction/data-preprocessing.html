
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>(Step 2) Data Preprocessing &#8212; Explainable AI for Software Engineering</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://xai4se.github.io/defect-prediction/data-preprocessing.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="(Step 3) Model Construction" href="model-construction.html" />
    <link rel="prev" title="(Step 1) Data Collection" href="data-collection.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explainable AI for Software Engineering</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Explainable AI for Software Engineering
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Hands-on Exercise
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/pyexplainer-live-demo.html">
   ASE2021 PyExplainer Live-Demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/hands-on-exercise.html">
   ASE2021 Hands-on Exercise
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part1-What is Explainable AI?
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../xai/theory-of-explanations.html">
   A Theory of Explanations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai/techniques-for-generating-explanations.html">
   Techniques for Generating Explanations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai/model-specific-techniques.html">
   Model-specific Techniques for Generating Global Explanations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai/model-agnostic-techniques.html">
   Model-agnostic Techniques for Generating Local Explanations
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part2-Defect Prediction Models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="software-quality-assurance.html">
   Software Quality Assurance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="defect-prediction.html">
   Defect Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-collection.html">
   (Step 1) Data Collection
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   (Step 2) Data Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-construction.html">
   (Step 3) Model Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-evaluation.html">
   (Step 4) Model Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-ranking.html">
   (Step 5) Model Ranking
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part3-Explainable AI for Software Engineering
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/explainability-for-se.html">
   Explainability in Software Engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/a-case-study-of-defect-prediction.html">
   A Case Study of Defect Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/defective-line-localization.html">
   (Example 1) Help developers localize which lines of code are the most risky
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/local-defect-explanation.html">
   (Example 2) Help developers understand why a file is predicted as defective
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/specific-sq-plan.html">
   (Example 3) Help managers develop software quality improvement plans
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/defect-prediction/data-preprocessing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xai4se/xai4se.github.io"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xai4se/xai4se.github.io/issues/new?title=Issue%20on%20page%20%2Fdefect-prediction/data-preprocessing.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/xai4se/xai4se.github.io/edit/master/docs/defect-prediction/data-preprocessing.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/xai4se/xai4se.github.io/master?urlpath=lab/tree/docs/defect-prediction/data-preprocessing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/xai4se/xai4se.github.io/blob/master/docs/defect-prediction/data-preprocessing.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#always-handle-collinearity-and-multicollinearity">
   Always handle collinearity and multicollinearity
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collinearity-pairwise-correlation">
     Collinearity (Pairwise Correlation)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multicollinearity">
     Multicollinearity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-selection-techniques-often-do-not-mitigate-collinearity-and-multicollinearity">
     Feature selection techniques often do not mitigate collinearity and multicollinearity!
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#filter-based-family">
       Filter-based Family
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#wrapper-based-family">
       Wrapper-based Family
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autospearman-an-automated-feature-selection-approach-that-address-collinearity-and-multicollinearity">
     AutoSpearman: An automated feature selection approach that address collinearity and multicollinearity
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#always-handle-class-imbalance">
   Always handle class imbalance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-rebalancing-techniques-for-software-analytics">
     Class Rebalancing Techniques for Software Analytics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#over-sampling-technique-over">
       Over-Sampling Technique (OVER)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#under-sampling-technique-under">
       Under-Sampling Technique (UNDER)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#synthetic-minority-oversampling-technique-smote">
       Synthetic Minority Oversampling Technique (SMOTE)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#suggested-readings">
   Suggested Readings
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="step-2-data-preprocessing">
<h1>(Step 2) Data Preprocessing<a class="headerlink" href="#step-2-data-preprocessing" title="Permalink to this headline">¶</a></h1>
<div class="section" id="always-handle-collinearity-and-multicollinearity">
<h2>Always handle collinearity and multicollinearity<a class="headerlink" href="#always-handle-collinearity-and-multicollinearity" title="Permalink to this headline">¶</a></h2>
<div class="section" id="collinearity-pairwise-correlation">
<h3>Collinearity (Pairwise Correlation)<a class="headerlink" href="#collinearity-pairwise-correlation" title="Permalink to this headline">¶</a></h3>
<p>Collinearity is a phenomenon in which one metric can be linearly predicted by another metric.
There are several correlation tests that can detect collinearity between metrics. For example, Pearson correlation test, Spearman correlation test, and Kendall Tau correlation test.
Below, we provide a tutorial for using and visualising Spearman correlation test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for Correlation tests</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Prepare a dataframe for collinearity simulation</span>
<span class="n">X_Corr</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Simulate a collinearity situation of AddedLOC and A</span>
<span class="n">X_Corr</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_Corr</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>


<span class="c1"># There are 3 options for the parameter setting of method as follows:</span>
<span class="c1"># pearson : standard correlation coefficient</span>
<span class="c1"># kendall : Kendall Tau correlation coefficient</span>
<span class="c1"># spearman : Spearman rank correlation</span>
<span class="n">corrmat</span> <span class="o">=</span> <span class="n">X_Corr</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;spearman&#39;</span><span class="p">)</span>
<span class="n">top_corr_features</span> <span class="o">=</span> <span class="n">corrmat</span><span class="o">.</span><span class="n">index</span>


<span class="c1"># Visualise a lower-triangle correlation heatmap</span>
<span class="n">mask_df</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">corrmat</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="c1">#plot heat map</span>
<span class="n">g</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X_Corr</span><span class="p">[</span><span class="n">top_corr_features</span><span class="p">]</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> 
              <span class="n">mask</span> <span class="o">=</span> <span class="n">mask_df</span><span class="p">,</span> 
              <span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">vmax</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
              <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;RdBu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/data-preprocessing_3_0.png" src="../_images/data-preprocessing_3_0.png" />
</div>
</div>
</div>
<div class="section" id="multicollinearity">
<h3>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline">¶</a></h3>
<p>Multicollinearity is a phenomenon in which one metric can be linearly predicted by a combination of two or more metrics.
Multicollinearity can be detected using Variance Inflation Factor (VIF) analysis <span id="id1">[<a class="reference internal" href="../References.html#id70">FM92</a>]</span>.
The idea behind variance inflation factor analysis is to construct an ordinary least square regression to predict a metric by using the other metrics in the dataset.
Having a model that is well fit indicates that the metric can be predicted by other metrics, linearly highly-correlated with other metrics.
There are 3 steps in variance inflation factor analysis.</p>
<p><em>(Step 1) Construct a regression model for each metric.</em>
For each metric, we construct a model using the other metrics to predict that particular metric.</p>
<p><em>(Step 2) Compute a VIF score for each metric.</em>
The VIF score for each metric is computed using the following formula: <span class="math notranslate nohighlight">\(\mathrm{VIF} = \frac{1}{1 - \mathrm{R}^2}\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{R}^2\)</span> is the explanatory power of the regression model from Step 1.
A high VIF score of a metric indicates that a given metric can be accurately predicted by the other metrics.
Thus, that given metric is considered redundant and should be removed from our model.</p>
<p><em>(Step 3) Remove metrics with a VIF score that is higher than a given threshold.</em>
We remove metrics with a VIF score that is higher than a given threshold.
We use a VIF threshold of 5 to determine the magnitude of multi-collinearity, as it is suggested by Fox <span id="id2">[<a class="reference internal" href="../References.html#id71">Fox15</a>]</span>.
Then, we repeat the above three steps until the VIF scores of all remaining metrics are lower than the pre-defined threshold.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for VIF</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">from</span> <span class="nn">statsmodels.tools.tools</span> <span class="kn">import</span> <span class="n">add_constant</span>


<span class="c1"># Prepare a dataframe for VIF</span>
<span class="n">X_VIF</span> <span class="o">=</span> <span class="n">add_constant</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Simulate a multicollinearity situation of AddedLOC, A, and B</span>
<span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>
<span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>

<span class="c1"># Calculate VIF scores</span>
<span class="n">vif_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X_VIF</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> 
               <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_VIF</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])],</span> 
              <span class="n">index</span><span class="o">=</span><span class="n">X_VIF</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="c1"># Prepare a final dataframe of VIF scores</span>
<span class="n">vif_scores</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">vif_scores</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="s1">&#39;VIFscore&#39;</span><span class="p">]</span>
<span class="n">vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">vif_scores</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;VIFscore&#39;</span><span class="p">],</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">vif_scores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature</th>
      <th>VIFscore</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>AddedLOC</td>
      <td>3.363851e+06</td>
    </tr>
    <tr>
      <th>7</th>
      <td>B</td>
      <td>2.421962e+06</td>
    </tr>
    <tr>
      <th>6</th>
      <td>A</td>
      <td>1.078763e+06</td>
    </tr>
    <tr>
      <th>3</th>
      <td>nCoupledClass</td>
      <td>1.220642e+00</td>
    </tr>
    <tr>
      <th>5</th>
      <td>CommentToCodeRatio</td>
      <td>1.126280e+00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>nCommit</td>
      <td>1.044982e+00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>LOC</td>
      <td>1.019733e+00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In this example, we observe that LOC, A, and B are problematic with the VIF scores of above 5.
To mitigate multicollinearity, we need to exclude one metric with the highest VIF score, i.e., LOC.
We iteratively repeat the process as described above.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for VIF</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">from</span> <span class="nn">statsmodels.tools.tools</span> <span class="kn">import</span> <span class="n">add_constant</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Prepare a dataframe for VIF</span>
<span class="n">X_VIF</span> <span class="o">=</span> <span class="n">add_constant</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Simulate a multicollinearity situation of AddedLOC, A, and B</span>
<span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>
<span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_VIF</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>


<span class="n">selected_features</span> <span class="o">=</span> <span class="n">X_VIF</span><span class="o">.</span><span class="n">columns</span>

<span class="c1"># Stepwise-VIF</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stepwise VIF START&#39;</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># Calculate VIF scores</span>
    <span class="n">vif_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X_VIF</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> 
                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_VIF</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])],</span> 
                  <span class="n">index</span><span class="o">=</span><span class="n">X_VIF</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="c1"># Prepare a final dataframe of VIF scores</span>
    <span class="n">vif_scores</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">vif_scores</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="s1">&#39;VIFscore&#39;</span><span class="p">]</span>
    <span class="n">vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">vif_scores</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">vif_scores</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;VIFscore&#39;</span><span class="p">],</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Find features that have their VIF scores of above 5.0</span>
    <span class="n">filtered_vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="p">[</span><span class="n">vif_scores</span><span class="p">[</span><span class="s1">&#39;VIFscore&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">5.0</span><span class="p">]</span>
    
    <span class="c1"># Terminate when there is no features with the VIF scores of above 5.0</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">filtered_vif_scores</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">break</span>
        
    <span class="c1"># exclude the metric with the highest VIF score</span>
    <span class="n">metric_to_exclude</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">filtered_vif_scores</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span><span class="s1">&#39;- exclude&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">metric_to_exclude</span><span class="p">))</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>
        
    <span class="n">selected_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">selected_features</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">metric_to_exclude</span><span class="p">]))</span>
    
    <span class="n">X_VIF</span> <span class="o">=</span> <span class="n">X_VIF</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">selected_features</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The following features are selected according to Stepwise VIF with the VIF threshold value of 5:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vif_scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stepwise VIF START
Step 1 - exclude AddedLOC
Step 2 - exclude B
The following features are selected according to Stepwise VIF with the VIF threshold value of 5:
              Feature  VIFscore
4       nCoupledClass  1.220647
5                   A  1.122400
2  CommentToCodeRatio  1.120934
3             nCommit  1.039827
0                 LOC  1.017052
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="feature-selection-techniques-often-do-not-mitigate-collinearity-and-multicollinearity">
<h3>Feature selection techniques often do not mitigate collinearity and multicollinearity!<a class="headerlink" href="#feature-selection-techniques-often-do-not-mitigate-collinearity-and-multicollinearity" title="Permalink to this headline">¶</a></h3>
<p>Feature selection is a data preprocessing technique for selecting a
subset of the best software metrics prior to constructing a defect
model. There is a plethora of feature selection techniques that can be
applied <span id="id3">[<a class="reference internal" href="../References.html#id84">GE03</a>]</span>, e.g., filter-based, wrapper-based, and
embedded-based families.</p>
<div class="section" id="filter-based-family">
<h4>Filter-based Family<a class="headerlink" href="#filter-based-family" title="Permalink to this headline">¶</a></h4>
<p>Filter-based feature selection techniques search for the best subset of
metrics according to an evaluation criterion regardless of model
construction. Since constructing models is not required, the use of
filter-based feature selection techniques is considered low cost and
widely used.</p>
<p><strong>Chi-Squared-based feature selection</strong> <span id="id4">[<a class="reference internal" href="../References.html#id192">McH13</a>]</span> assesses the
importance of metrics with the <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic which is a
non-parametric statistical test of independence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for Chi-sqaured-based feature selection technique</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">chi2</span>

<span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">chi2_fs</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dfscores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">chi2_fs</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="n">dfcolumns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="c1">#concat two dataframes for better visualization </span>
<span class="n">featureScores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">dfcolumns</span><span class="p">,</span><span class="n">dfscores</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">featureScores</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span><span class="s1">&#39;Score&#39;</span><span class="p">]</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top-k features (k =&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">top_k</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;) according to Chi-squared statistics are as follows:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">featureScores</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span><span class="s1">&#39;Score&#39;</span><span class="p">))</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Top-k features (k =3) according to Chi-squared statistics are as follows:
              Feature         Score
1            AddedLOC  26834.488115
2       nCoupledClass   1599.996258
4  CommentToCodeRatio     64.268658
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="wrapper-based-family">
<h4>Wrapper-based Family<a class="headerlink" href="#wrapper-based-family" title="Permalink to this headline">¶</a></h4>
<p>Wrapper-based feature selection
techniques <span id="id5">[<a class="reference internal" href="../References.html#id129">JKP94</a>]</span><span id="id6">[<a class="reference internal" href="../References.html#id150">KJ97</a>]</span> use classification
techniques to assess each subset of metrics and find the best subset of
metrics according to an evaluation criterion. Wrapper-based feature
selection is made up of three steps, which we described below.</p>
<p><em>(Step 1) Generate a subset of metrics.</em> Since it is impossible to
evaluate all possible subsets of metrics, wrapper-based feature
selection often uses search techniques (e.g., best first, greedy hill
climbing) to generate candidate subsets of metrics for evaluation.</p>
<p><em>(Step 2) Construct a classifier using a subset of metrics with a
predetermined classification technique.</em> Wrapper-based feature selection
constructs a classification model using a candidate subset of metrics
for a given classification technique (e.g., logistic regression and
random forest).</p>
<p><em>(Step 3) Evaluate the classifier according to a given evaluation
criterion.</em> Once the classifier is constructed, wrapper-based feature
selection evaluates the classifier using a given evaluation criterion
(e.g., Akaike Information Criterion).</p>
<p>For each candidate subset of metrics, wrapper-based feature selection
repeats Steps 2 and 3 in order to find the best subset of metrics
according to the evaluation criterion. Finally, it provides the best
subset of metrics that yields the highest performance according to the
evaluation criterion.</p>
<p><strong>Recursive Feature Elimination</strong> (RFE) <span id="id7">[<a class="reference internal" href="../References.html#id84">GE03</a>]</span>
searches for the best subset of metrics by recursively eliminating the
least important metric. First, RFE constructs a model using all metrics
and ranks metrics according to their importance score (e.g., Breiman’s
Variable Importance for random forest). In each iteration, RFE excludes
the least important metric and reconstructs a model. Finally, RFE
provides the subset of metrics which yields the best performance
according to an evaluation criterion (e.g., AUC).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Feature Extraction with RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">compress</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">rf_model</span><span class="p">,</span> <span class="n">n_features_to_select</span> <span class="o">=</span> <span class="n">top_k</span><span class="p">)</span>
<span class="n">rfe_fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rfe_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">compress</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">rfe_fit</span><span class="o">.</span><span class="n">support_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top-k (k =&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">top_k</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;) according to the RFE teachnique are as follows:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rfe_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Top-k (k =3) according to the RFE teachnique are as follows:
[&#39;AddedLOC&#39;, &#39;nCoupledClass&#39;, &#39;CommentToCodeRatio&#39;]
</pre></div>
</div>
</div>
</div>
<p>Below, we provide an interactive tutorial to show that feature selection techniques may not mitigate collinearity and multicollinearity.</p>
<p>In this tutorial, we stimulate a multicollinearity situation of <code class="docutils literal notranslate"><span class="pre">AddedLOC</span></code>, <code class="docutils literal notranslate"><span class="pre">A</span></code>, and <code class="docutils literal notranslate"><span class="pre">B</span></code>.
However, all of these highly-correlated metrics with multicollinearity are selected by the Chi-squared feature selection technique.
In addition, according to the Recursive Feature Elimination technique, <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> are among the top-3 metrics that are selected.
These findings suggest that feature selection techniques may select highly-correlated metrics and do not mitigate collinearity and multicollinearity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare a dataframe for simulation</span>
<span class="n">X_simulation_train</span> <span class="o">=</span> <span class="n">add_constant</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Simulate a multicollinearity situation of AddedLOC, A, and B</span>
<span class="n">X_simulation_train</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_simulation_train</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>
<span class="n">X_simulation_train</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_simulation_train</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>

<span class="c1"># apply the Chi-squared feature selection technique</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">chi2_fs</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_simulation_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dfscores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">chi2_fs</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="n">dfcolumns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_simulation_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="c1">#concat two dataframes for better visualization </span>
<span class="n">featureScores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">dfcolumns</span><span class="p">,</span><span class="n">dfscores</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">featureScores</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span><span class="s1">&#39;Score&#39;</span><span class="p">]</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top-k features (k =&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">top_k</span><span class="p">)</span> <span class="o">+</span><span class="s1">&#39;) according to Chi-squared statistics are as follows:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">featureScores</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span><span class="s1">&#39;Score&#39;</span><span class="p">))</span> 

<span class="c1"># apply the Recursive Feature Elimination technique</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">rf_model</span><span class="p">,</span> <span class="n">n_features_to_select</span> <span class="o">=</span> <span class="n">top_k</span><span class="p">)</span>
<span class="n">rfe_fit</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_simulation_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rfe_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">compress</span><span class="p">(</span><span class="n">X_simulation_train</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">rfe_fit</span><span class="o">.</span><span class="n">support_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top-k (k =&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">top_k</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;) according to the RFE teachnique are as follows:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rfe_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Top-k features (k =3) according to Chi-squared statistics are as follows:
    Feature         Score
7         B  80329.360766
6         A  53507.874728
2  AddedLOC  26834.488115
Top-k (k =3) according to the RFE teachnique are as follows:
[&#39;nCoupledClass&#39;, &#39;A&#39;, &#39;B&#39;]
</pre></div>
</div>
</div>
</div>
<p>To handle collinearity and multicollinearity, correlation analysis techniques (e.g., Spearman rank correlation test and Variance Inflation Factor analysis) should be used.
However, these correlation analysis techniques often involve manual process.
For example, in the above tutorial (Collinearity), the Spearman rank correlation test was used to measure the pair-wise correlation among metrics and presented using the heatmap plot.
In the tutorial, we found that <code class="docutils literal notranslate"><span class="pre">AddedLOC</span></code> and <code class="docutils literal notranslate"><span class="pre">A</span></code> are highly-correlated with the Spearman correlation of 1 (perfect correlation).
To mitigate this, we have to select one of them but the question is <em>Which one shoud be selected to mitigate collinearity?</em></p>
</div>
</div>
<div class="section" id="autospearman-an-automated-feature-selection-approach-that-address-collinearity-and-multicollinearity">
<h3>AutoSpearman: An automated feature selection approach that address collinearity and multicollinearity<a class="headerlink" href="#autospearman-an-automated-feature-selection-approach-that-address-collinearity-and-multicollinearity" title="Permalink to this headline">¶</a></h3>
<p>Jiarpakdee <em>et al.</em> <span id="id8">[<a class="reference internal" href="../References.html#id121">JTT18a</a>]</span><span id="id9">[<a class="reference internal" href="../References.html#id125">JTT20b</a>]</span> <span id="id10">[<a class="reference internal" href="../References.html#id118">JTT18b</a>]</span> introduce , an
automated metric selection approach based on the Spearman rank
correlation test and the VIF analysis for statistical inference.
The high-level concept of can be summarised into 2 parts:</p>
<p><em>(Part 1) Automatically select non-correlated metrics based on a
Spearman rank correlation test.</em> We first measure the correlation of all
metrics using the Spearman rank correlation test (<span class="math notranslate nohighlight">\(\rho\)</span>). We use the interpretation of correlation coefficients (<span class="math notranslate nohighlight">\(|\rho|\)</span>) as
provided by Kraemer <span id="id11">[<a class="reference internal" href="../References.html#id153">KML+03</a>]</span>—i.e., a Spearman
correlation coefficient of above or equal to 0.7 is considered a strong
correlation. Thus, we only consider the pairs that have an absolute
Spearman correlation coefficient of above or equal to the threshold
value (<span class="math notranslate nohighlight">\(sp.t\)</span>) of 0.7.</p>
<p>To automatically select non-correlated metrics based on the Spearman
rank correlation test, we start from the pair that has the highest
Spearman correlation coefficient. Since the two
correlated metrics under examination can be linearly predicted with each
other, one of these two metrics must be removed. Thus, we select the
metric that has the lowest average values of the absolute Spearman
correlation coefficients of the other metrics that are not included in
the pair. That means the removed metric is another metric
in the pair that is not selected. Since the removed
metric may be correlated with the other metrics, we remove any pairs of
metrics that are correlated with the removed metric.
Finally, we exclude the removed metric from the set of the remaining
metrics (<span class="math notranslate nohighlight">\(M'\)</span>). We repeat this process until all pairs of
metrics have their Spearman correlation coefficient below a threshold
value of 0.7.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare a dataframe for AutoSpearman demo</span>
<span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Simulate a multicollinearity situation of AddedLOC, A, and B</span>
<span class="n">X_AS_train</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_AS_train</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>
<span class="n">X_AS_train</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x_i</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">X_AS_train</span><span class="p">[</span><span class="s1">&#39;AddedLOC&#39;</span><span class="p">]]</span>

<span class="n">AS_metrics</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">columns</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># (Part 1) Automatically select non-correlated metrics based on a Spearman rank correlation test.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(Part 1) Automatically select non-correlated metrics based on a Spearman rank correlation test&#39;</span><span class="p">)</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">corrmat</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;spearman&#39;</span><span class="p">)</span>
    <span class="n">top_corr_features</span> <span class="o">=</span> <span class="n">corrmat</span><span class="o">.</span><span class="n">index</span>
    <span class="n">abs_corrmat</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">corrmat</span><span class="p">)</span>
    
    <span class="c1"># identify correlated metrics with the correlation threshold of 0.7</span>
    <span class="n">highly_correlated_metrics</span> <span class="o">=</span> <span class="p">((</span><span class="n">corrmat</span> <span class="o">&gt;</span> <span class="mf">.7</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">corrmat</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">.7</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">corrmat</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">n_correlated_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">highly_correlated_metrics</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">n_correlated_metrics</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># find the strongest pair-wise correlation</span>
        <span class="n">find_top_corr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">abs_corrmat</span><span class="p">,</span> <span class="n">ignore_index</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">find_top_corr</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">find_top_corr</span> <span class="o">=</span> <span class="n">find_top_corr</span><span class="p">[</span><span class="n">find_top_corr</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">top_corr_index</span> <span class="o">=</span> <span class="n">find_top_corr</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
        <span class="n">top_corr_i</span> <span class="o">=</span> <span class="n">find_top_corr</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">top_corr_index</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># get the 2 correlated metrics with the strongest correlation</span>
        <span class="n">correlated_metric_1</span> <span class="o">=</span> <span class="n">top_corr_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">correlated_metric_2</span> <span class="o">=</span> <span class="n">top_corr_i</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span><span class="s1">&#39;comparing between&#39;</span><span class="p">,</span> <span class="n">correlated_metric_1</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">)</span>
        
        <span class="c1"># compute their correlation with other metrics outside of the pair</span>
        <span class="n">correlation_with_other_metrics_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_corrmat</span><span class="p">[</span><span class="n">correlated_metric_1</span><span class="p">][[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_corr_features</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">correlated_metric_1</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">]]])</span>
        <span class="n">correlation_with_other_metrics_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_corrmat</span><span class="p">[</span><span class="n">correlated_metric_2</span><span class="p">][[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_corr_features</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">correlated_metric_1</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">]]])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&#39;</span><span class="p">,</span> <span class="n">correlated_metric_1</span><span class="p">,</span> <span class="s1">&#39;has the average correlation of&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">correlation_with_other_metrics_1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;with other metrics&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&#39;</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">,</span> <span class="s1">&#39;has the average correlation of&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">correlation_with_other_metrics_2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="p">,</span> <span class="s1">&#39;with other metrics&#39;</span><span class="p">)</span>
        <span class="c1"># select the metric that shares the least correlation outside of the pair and exclude the other</span>
        <span class="k">if</span> <span class="n">correlation_with_other_metrics_1</span> <span class="o">&lt;</span> <span class="n">correlation_with_other_metrics_2</span><span class="p">:</span>
            <span class="n">exclude_metric</span> <span class="o">=</span> <span class="n">correlated_metric_2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">exclude_metric</span> <span class="o">=</span> <span class="n">correlated_metric_1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;Exclude&#39;</span><span class="p">,</span><span class="n">exclude_metric</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span>
        <span class="n">AS_metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">AS_metrics</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">exclude_metric</span><span class="p">]))</span>
        <span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="p">[</span><span class="n">AS_metrics</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;According to Part 1 of AutoSpearman,&#39;</span><span class="p">,</span> <span class="n">AS_metrics</span><span class="p">,</span><span class="s1">&#39;are selected.&#39;</span><span class="p">)</span>
<span class="n">generate_heatmap</span><span class="p">(</span><span class="n">X_AS_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Part 1) Automatically select non-correlated metrics based on a Spearman rank correlation test
Step 1 comparing between A and AddedLOC
&gt; A has the average correlation of 0.382 with other metrics
&gt; AddedLOC has the average correlation of 0.387 with other metrics
&gt; Exclude AddedLOC
Step 2 comparing between A and B
&gt; A has the average correlation of 0.231 with other metrics
&gt; B has the average correlation of 0.235 with other metrics
&gt; Exclude B
According to Part 1 of AutoSpearman, [&#39;nCommit&#39;, &#39;CommentToCodeRatio&#39;, &#39;LOC&#39;, &#39;nCoupledClass&#39;, &#39;A&#39;] are selected.
</pre></div>
</div>
<img alt="../_images/data-preprocessing_16_1.png" src="../_images/data-preprocessing_16_1.png" />
</div>
</div>
<p><em>(Part 2) Automatically select non-correlated metrics based on a
Variance Inflation Factor analysis.</em> We first measure the magnitude of
multicollinearity of the remaining metrics (<span class="math notranslate nohighlight">\(M'\)</span>) from <code class="docutils literal notranslate"><span class="pre">Part</span> <span class="pre">1</span></code> using
the Variance Inflation Factor analysis. We use a VIF
threshold value (<span class="math notranslate nohighlight">\(vif.t\)</span>) of 5 to identify the presence of
multicollinearity, as suggested by Fox <span id="id12">[<a class="reference internal" href="../References.html#id71">Fox15</a>]</span>.</p>
<p>To automatically remove correlated metrics from the Variance Inflation
Factor analysis, we identify the removed metric as the metric that has
the highest VIF score. We then exclude the removed
metric from the set of the remaining metrics (<span class="math notranslate nohighlight">\(M'\)</span>). We
apply the VIF analysis on the remaining metrics until none of the
remaining metrics have their VIF scores above or equal to the threshold
value. Finally,  produces a subset of non-correlated
metrics based on the Spearman rank correlation test and the VIF analysis
(<span class="math notranslate nohighlight">\(M'\)</span>).</p>
<p>Similar to filter-based feature selection techniques, <em>Part 1</em>
of  measures the correlation of all metrics using the Spearman rank
correlation test regardless of model construction. Similar to
wrapper-based feature selection techniques, <em>Part 2</em> of  constructs
regression models to measure the magnitude of multicollinearity of
metrics. Thus, we consider  as a hybrid feature selection technique
(both filter-based and wrapper-based).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare a dataframe for VIF</span>
<span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">add_constant</span><span class="p">(</span><span class="n">X_AS_train</span><span class="p">)</span>

<span class="n">selected_features</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">columns</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># (Part 2) Automatically select non-correlated metrics based on a Variance Inflation Factor analysis.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(Part 2) Automatically select non-correlated metrics based on a Variance Inflation Factor analysis&#39;</span><span class="p">)</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># Calculate VIF scores</span>
    <span class="n">vif_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X_AS_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> 
                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_AS_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])],</span> 
                  <span class="n">index</span><span class="o">=</span><span class="n">X_AS_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="c1"># Prepare a final dataframe of VIF scores</span>
    <span class="n">vif_scores</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">vif_scores</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="s1">&#39;VIFscore&#39;</span><span class="p">]</span>
    <span class="n">vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">vif_scores</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">vif_scores</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;VIFscore&#39;</span><span class="p">],</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Find features that have their VIF scores of above 5.0</span>
    <span class="n">filtered_vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="p">[</span><span class="n">vif_scores</span><span class="p">[</span><span class="s1">&#39;VIFscore&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">5.0</span><span class="p">]</span>
    
    <span class="c1"># Terminate when there is no features with the VIF scores of above 5.0</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">filtered_vif_scores</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">break</span>
        
    <span class="c1"># exclude the metric with the highest VIF score</span>
    <span class="n">metric_to_exclude</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">filtered_vif_scores</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span><span class="s1">&#39;- exclude&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">metric_to_exclude</span><span class="p">))</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>
        
    <span class="n">selected_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">selected_features</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">metric_to_exclude</span><span class="p">]))</span>
    
    <span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">selected_features</span><span class="p">]</span>

    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Finally, according to Part 2 of AutoSpearman,&#39;</span><span class="p">,</span> <span class="n">AS_metrics</span><span class="p">,</span><span class="s1">&#39;are selected.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Part 2) Automatically select non-correlated metrics based on a Variance Inflation Factor analysis
Finally, according to Part 2 of AutoSpearman, [&#39;nCommit&#39;, &#39;CommentToCodeRatio&#39;, &#39;LOC&#39;, &#39;nCoupledClass&#39;, &#39;A&#39;] are selected.
</pre></div>
</div>
</div>
</div>
<p>To foster future replication, we provide a python implementation of AutoSpearman as a function below.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for AutoSpearman</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">from</span> <span class="nn">statsmodels.tools.tools</span> <span class="kn">import</span> <span class="n">add_constant</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    For more detail and citation, please refer to:</span>
<span class="sd">    [1] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Christoph Treude:</span>
<span class="sd">    The Impact of Automated Feature Selection Techniques on the Interpretation of Defect Models. Empir. Softw. Eng. 25(5): 3590-3638 (2020)</span>

<span class="sd">    [2] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Christoph Treude:</span>
<span class="sd">    AutoSpearman: Automatically Mitigating Correlated Software Metrics for Interpreting Defect Models. ICSME 2018: 92-103</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">AutoSpearman</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">correlation_threshold</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">correlation_method</span> <span class="o">=</span> <span class="s1">&#39;spearman&#39;</span><span class="p">,</span> <span class="n">VIF_threshold</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    
    <span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">AS_metrics</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">columns</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="c1"># (Part 1) Automatically select non-correlated metrics based on a Spearman rank correlation test.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(Part 1) Automatically select non-correlated metrics based on a Spearman rank correlation test&#39;</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">corrmat</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="n">correlation_method</span><span class="p">)</span>
        <span class="n">top_corr_features</span> <span class="o">=</span> <span class="n">corrmat</span><span class="o">.</span><span class="n">index</span>
        <span class="n">abs_corrmat</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">corrmat</span><span class="p">)</span>

        <span class="c1"># identify correlated metrics with the correlation threshold of the threshold</span>
        <span class="n">highly_correlated_metrics</span> <span class="o">=</span> <span class="p">((</span><span class="n">corrmat</span> <span class="o">&gt;</span> <span class="n">correlation_threshold</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">corrmat</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">correlation_threshold</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">corrmat</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">n_correlated_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">highly_correlated_metrics</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">n_correlated_metrics</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># find the strongest pair-wise correlation</span>
            <span class="n">find_top_corr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">abs_corrmat</span><span class="p">,</span> <span class="n">ignore_index</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">find_top_corr</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">find_top_corr</span> <span class="o">=</span> <span class="n">find_top_corr</span><span class="p">[</span><span class="n">find_top_corr</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">top_corr_index</span> <span class="o">=</span> <span class="n">find_top_corr</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
            <span class="n">top_corr_i</span> <span class="o">=</span> <span class="n">find_top_corr</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">top_corr_index</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># get the 2 correlated metrics with the strongest correlation</span>
            <span class="n">correlated_metric_1</span> <span class="o">=</span> <span class="n">top_corr_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">correlated_metric_2</span> <span class="o">=</span> <span class="n">top_corr_i</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt; Step&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span><span class="s1">&#39;comparing between&#39;</span><span class="p">,</span> <span class="n">correlated_metric_1</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">)</span>

            <span class="c1"># compute their correlation with other metrics outside of the pair</span>
            <span class="n">correlation_with_other_metrics_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_corrmat</span><span class="p">[</span><span class="n">correlated_metric_1</span><span class="p">][[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_corr_features</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">correlated_metric_1</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">]]])</span>
            <span class="n">correlation_with_other_metrics_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_corrmat</span><span class="p">[</span><span class="n">correlated_metric_2</span><span class="p">][[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_corr_features</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">correlated_metric_1</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">]]])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&gt;&#39;</span><span class="p">,</span> <span class="n">correlated_metric_1</span><span class="p">,</span> <span class="s1">&#39;has the average correlation of&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">correlation_with_other_metrics_1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;with other metrics&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&gt;&#39;</span><span class="p">,</span> <span class="n">correlated_metric_2</span><span class="p">,</span> <span class="s1">&#39;has the average correlation of&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">correlation_with_other_metrics_2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="p">,</span> <span class="s1">&#39;with other metrics&#39;</span><span class="p">)</span>
            <span class="c1"># select the metric that shares the least correlation outside of the pair and exclude the other</span>
            <span class="k">if</span> <span class="n">correlation_with_other_metrics_1</span> <span class="o">&lt;</span> <span class="n">correlation_with_other_metrics_2</span><span class="p">:</span>
                <span class="n">exclude_metric</span> <span class="o">=</span> <span class="n">correlated_metric_2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">exclude_metric</span> <span class="o">=</span> <span class="n">correlated_metric_1</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;Exclude&#39;</span><span class="p">,</span><span class="n">exclude_metric</span><span class="p">)</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span>
            <span class="n">AS_metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">AS_metrics</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">exclude_metric</span><span class="p">]))</span>
            <span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="p">[</span><span class="n">AS_metrics</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;According to Part 1 of AutoSpearman,&#39;</span><span class="p">,</span> <span class="n">AS_metrics</span><span class="p">,</span><span class="s1">&#39;are selected.&#39;</span><span class="p">)</span>

    <span class="c1"># (Part 2) Automatically select non-correlated metrics based on a Variance Inflation Factor analysis.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(Part 2) Automatically select non-correlated metrics based on a Variance Inflation Factor analysis&#39;</span><span class="p">)</span>
    
    <span class="c1"># Prepare a dataframe for VIF</span>
    <span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">add_constant</span><span class="p">(</span><span class="n">X_AS_train</span><span class="p">)</span>

    <span class="n">selected_features</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">columns</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Calculate VIF scores</span>
        <span class="n">vif_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X_AS_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> 
                       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_AS_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])],</span> 
                      <span class="n">index</span><span class="o">=</span><span class="n">X_AS_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="c1"># Prepare a final dataframe of VIF scores</span>
        <span class="n">vif_scores</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">vif_scores</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="s1">&#39;VIFscore&#39;</span><span class="p">]</span>
        <span class="n">vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">vif_scores</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">vif_scores</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;VIFscore&#39;</span><span class="p">],</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Find features that have their VIF scores of above the threshold</span>
        <span class="n">filtered_vif_scores</span> <span class="o">=</span> <span class="n">vif_scores</span><span class="p">[</span><span class="n">vif_scores</span><span class="p">[</span><span class="s1">&#39;VIFscore&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">VIF_threshold</span><span class="p">]</span>

        <span class="c1"># Terminate when there is no features with the VIF scores of above the threshold</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">filtered_vif_scores</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># exclude the metric with the highest VIF score</span>
        <span class="n">metric_to_exclude</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">filtered_vif_scores</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt; Step&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span><span class="s1">&#39;- exclude&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">metric_to_exclude</span><span class="p">))</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">selected_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">selected_features</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">metric_to_exclude</span><span class="p">]))</span>

        <span class="n">X_AS_train</span> <span class="o">=</span> <span class="n">X_AS_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">selected_features</span><span class="p">]</span>


    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Finally, according to Part 2 of AutoSpearman,&#39;</span><span class="p">,</span> <span class="n">AS_metrics</span><span class="p">,</span><span class="s1">&#39;are selected.&#39;</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">AS_metrics</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="always-handle-class-imbalance">
<h2>Always handle class imbalance<a class="headerlink" href="#always-handle-class-imbalance" title="Permalink to this headline">¶</a></h2>
<div class="section" id="class-rebalancing-techniques-for-software-analytics">
<h3>Class Rebalancing Techniques for Software Analytics<a class="headerlink" href="#class-rebalancing-techniques-for-software-analytics" title="Permalink to this headline">¶</a></h3>
<p>A plethora of class rebalancing techniques exist <span id="id13">[<a class="reference internal" href="../References.html#id100">HG09</a>]</span>,
e.g., (1) sampling methods for imbalanced learning, (2) cost-sensitive
methods for imbalanced learning, (3) kernel-based methods for imbalanced
learning, and (4) active learning for imbalanced learning. Since it is
impractical to study all of these techniques, we select a manageable set
of class rebalancing techniques for our study. As discussed by
He <span id="id14">[<a class="reference internal" href="../References.html#id100">HG09</a>]</span>, we start from the four families of imbalance
learning techniques. Based on a literature surveys by Hall <em>et al.</em> <span id="id15">[<a class="reference internal" href="../References.html#id89">HBB+12</a>]</span>,
Shihab <span id="id16">[<a class="reference internal" href="../References.html#id278">Shi12</a>]</span>, and Nam <span id="id17">[<a class="reference internal" href="../References.html#id228">Nam14</a>]</span>, we then select only the
family of sampling techniques for the context of defect prediction.</p>
<p>We first select the three commonly-used techniques (i.e., over-sampling,
under-sampling, and Default SMOTE <span id="id18">[<a class="reference internal" href="../References.html#id42">CBHK02</a>]</span>) that were
previously used in the literature <span id="id19">[<a class="reference internal" href="../References.html#id134">KameiMondenMatsumoto+07</a>]</span><span id="id20">[<a class="reference internal" href="../References.html#id143">KGS10</a>]</span><span id="id21">[<a class="reference internal" href="../References.html#id240">PD07</a>]</span><span id="id22">[<a class="reference internal" href="../References.html#id273">SKVanHulseF14</a>]</span><span id="id23">[<a class="reference internal" href="../References.html#id289">TTDM15</a>]</span><span id="id24">[<a class="reference internal" href="../References.html#id322">WY13</a>]</span><span id="id25">[<a class="reference internal" href="../References.html#id333">XLS+15</a>]</span><span id="id26">[<a class="reference internal" href="../References.html#id335">YLH+16</a>]</span><span id="id27">[<a class="reference internal" href="../References.html#id336">YLX+17</a>]</span></p>
<div class="figure align-default" id="class-imbalance-overview">
<img alt="../_images/class-imbalance-overview.png" src="../_images/class-imbalance-overview.png" />
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">An illustrative overview of the 3 selected class rebalancing techniques.</span><a class="headerlink" href="#class-imbalance-overview" title="Permalink to this image">¶</a></p>
</div>
<p>Below, we provide a description and a discussion of the 3 selected class rebalancing techniques for our book with interactive python tutorials.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find a class ratio of the original training dataset</span>
<span class="n">get_class_ratio</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="over-sampling-technique-over">
<h4>Over-Sampling Technique (OVER)<a class="headerlink" href="#over-sampling-technique-over" title="Permalink to this headline">¶</a></h4>
<p>The over-sampling technique (a.k.a. up-sampling) randomly samples with
replacement (i.e., replicating) <em>the minority class</em> (e.g., defective
class) to be the same size as the majority class (e.g., clean class).
The advantage of an over-sampling technique is that it leads to no
information loss. Since oversampling simply adds replicated modules from
the original dataset, the disadvantage is that the training dataset ends
up with multiple redundant modules, leading to an overfitting. Thus,
when applying the over-sampling technique, the performance of with-in
defect prediction models is likely higher than the performance of
cross-project defect prediction models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for the Over-Sampling technique (OVER)</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span>

<span class="c1"># Apply the Over-Sampling technique</span>
<span class="n">oversample</span> <span class="o">=</span> <span class="n">RandomOverSampler</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">&#39;minority&#39;</span><span class="p">)</span>
<span class="n">X_OVER_train</span><span class="p">,</span> <span class="n">y_OVER_train</span> <span class="o">=</span> <span class="n">oversample</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Find a class ratio of the over-sampled training dataset</span>
<span class="n">get_class_ratio</span><span class="p">(</span><span class="n">y_OVER_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="under-sampling-technique-under">
<h4>Under-Sampling Technique (UNDER)<a class="headerlink" href="#under-sampling-technique-under" title="Permalink to this headline">¶</a></h4>
<p>The under-sampling technique (a.k.a. down-sampling) randomly samples
(i.e., reducing) <em>the majority class</em> (e.g., clean class) in order to
reduce the number of majority modules to be the same number as the
minority class (e.g., defective class). The advantage of an
under-sampling technique is that it reduces the size of the training
data when the original data is relatively large. However, the
disadvantage is that removing modules may cause the training data to
lose important information pertaining to the majority class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for the Under-Sampling technique (UNDER)</span>
<span class="kn">from</span> <span class="nn">imblearn.under_sampling</span> <span class="kn">import</span> <span class="n">RandomUnderSampler</span>

<span class="c1"># Apply the Under-Sampling technique</span>
<span class="n">undersample</span> <span class="o">=</span> <span class="n">RandomUnderSampler</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">&#39;majority&#39;</span><span class="p">)</span>
<span class="n">X_UNDER_train</span><span class="p">,</span> <span class="n">y_UNDER_train</span> <span class="o">=</span> <span class="n">undersample</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Find a class ratio of the under-sampled training dataset</span>
<span class="n">get_class_ratio</span><span class="p">(</span><span class="n">y_UNDER_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="synthetic-minority-oversampling-technique-smote">
<h4>Synthetic Minority Oversampling Technique (SMOTE)<a class="headerlink" href="#synthetic-minority-oversampling-technique-smote" title="Permalink to this headline">¶</a></h4>
<p>The SMOTE technique <span id="id28">[<a class="reference internal" href="../References.html#id42">CBHK02</a>]</span> was proposed to combat the
disavantages of the simple over-sampling and under-sampling techniques.
The SMOTE technique creates artificial data based on the feature space
(rather than the data space) similarities from the minority modules. The
SMOTE technique starts with a set of minority modules (i.e., defective
modules). For each of the minority defective modules of the training
datasets, SMOTE performs the following steps:</p>
<ol class="simple">
<li><p>Calculate the <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbors.</p></li>
<li><p>Select <span class="math notranslate nohighlight">\(N\)</span> majority clean modules based on the smallest magnitude of
the euclidean distances that are obtained from the <span class="math notranslate nohighlight">\(k\)</span>-nearest
neighbors.</p></li>
</ol>
<p>Finally, SMOTE combines the synthetic oversampling of the minority
defective modules with the undersampling the majority clean modules.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import for the Synthetic Minority Oversampling Technique (SMOTE)</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>

<span class="c1"># Apply the SMOTE technique</span>
<span class="n">oversample_SMOTE</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">&#39;minority&#39;</span><span class="p">)</span>
<span class="n">X_SMOTE_train</span><span class="p">,</span> <span class="n">y_SMOTE_train</span> <span class="o">=</span> <span class="n">oversample_SMOTE</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Find a class ratio of the SMOTE-ed training dataset</span>
<span class="n">get_class_ratio</span><span class="p">(</span><span class="n">y_SMOTE_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parts of this chapter have been published by the following publications:\
[1] Chakkrit Tantithamthavorn, Ahmed E. Hassan, Kenichi Matsumoto: The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models. IEEE Trans. Software Eng. 46(11): 1200-1219 (2020). <br />
[2] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Ahmed E. Hassan: The Impact of Correlated Metrics on the Interpretation of Defect Models. IEEE Trans. Software Eng. 47(2): 320-331 (2021) <a class="reference external" href="https://doi.org/10.1109/TSE.2019.2891758.%22">https://doi.org/10.1109/TSE.2019.2891758.”</a></p>
</div>
</div>
</div>
</div>
<div class="section" id="suggested-readings">
<h2>Suggested Readings<a class="headerlink" href="#suggested-readings" title="Permalink to this headline">¶</a></h2>
<p>[1] Amritanshu Agrawal, Tim Menzies: Is “better data” better than “better data miners”?: on the benefits of tuning SMOTE for defect prediction. ICSE 2018: 1050-1061.</p>
<p>[2] Chakkrit Tantithamthavorn, Ahmed E. Hassan, Kenichi Matsumoto: The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models. IEEE Trans. Software Eng. 46(11): 1200-1219 (2020).</p>
<p>[3] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Ahmed E. Hassan: The Impact of Correlated Metrics on the Interpretation of Defect Models. IEEE Trans. Software Eng. 47(2): 320-331 (2021).</p>
<p>[4] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Christoph Treude:
The Impact of Automated Feature Selection Techniques on the Interpretation of Defect Models. Empir. Softw. Eng. 25(5): 3590-3638 (2020).</p>
<p>[5] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Christoph Treude:
AutoSpearman: Automatically Mitigating Correlated Software Metrics for Interpreting Defect Models. ICSME 2018: 92-103.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "xaitools"
        },
        kernelOptions: {
            kernelName: "xaitools",
            path: "./defect-prediction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'xaitools'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="data-collection.html" title="previous page">(Step 1) Data Collection</a>
    <a class='right-next' id="next-link" href="model-construction.html" title="next page">(Step 3) Model Construction</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Chakkrit Tantithamthavorn and Jirayus Jiarpakdee<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            <script>mermaid.init();</script> <div class="row_footer"> This project has received funding from the <a href="https://www.arc.gov.au/">Australian Research Council</a>'s Discovery Early Career Researcher Award (ARC DECRA) funding scheme (DE200100941). This book reflects the views of the authors and neither Australian Research Council nor Monash University are liable for any use that may be made of the information contained herein. The content of this project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The source code that is part of the content, as well as the source code used to format and display that content is licensed under the MIT License. </div>
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-54962993-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>