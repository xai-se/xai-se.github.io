{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always find the best subset of the most relevant software metrics\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "\n",
    "Feature selection is a data preprocessing technique for selecting a\n",
    "subset of the best software metrics prior to constructing a defect\n",
    "model. There is a plethora of feature selection techniques that can be\n",
    "applied {cite}`guyon2003introduction`, e.g., filter-based, wrapper-based, and\n",
    "embedded-based families. In this book, we would like to select a manageable set of feature\n",
    "selection techniques from filter-based and wrapper-based familites for preparing interactive tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Load Data and preparing datasets\n",
    "\n",
    "# Import for Load Data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "# Import for Split Data into Training and Testing Samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset = pd.read_csv((\"../../datasets/lucene-2.9.0.csv\"), index_col = 'File')\n",
    "test_dataset = pd.read_csv((\"../../datasets/lucene-3.0.0.csv\"), index_col = 'File')\n",
    "\n",
    "outcome = 'RealBug'\n",
    "features = ['OWN_COMMIT', 'Added_lines', 'CountClassCoupled', 'AvgLine', 'RatioCommentToCode']\n",
    "\n",
    "# process outcome to 0 and 1\n",
    "train_dataset[outcome] = pd.Categorical(train_dataset[outcome])\n",
    "train_dataset[outcome] = train_dataset[outcome].cat.codes\n",
    "\n",
    "test_dataset[outcome] = pd.Categorical(test_dataset[outcome])\n",
    "test_dataset[outcome] = test_dataset[outcome].cat.codes\n",
    "\n",
    "X_train = train_dataset.loc[:, features]\n",
    "X_test = test_dataset.loc[:, features]\n",
    "\n",
    "y_train = train_dataset.loc[:, outcome]\n",
    "y_test = test_dataset.loc[:, outcome]\n",
    "\n",
    "\n",
    "# commits - # of commits that modify the file of interest\n",
    "# Added lines - # of added lines of code\n",
    "# Count class coupled - # of classes that interact or couple with the class of interest\n",
    "# LOC - # of lines of code\n",
    "# RatioCommentToCode - The ratio of lines of comments to lines of code\n",
    "features = ['nCommit', 'AddedLOC', 'nCoupledClass', 'LOC', 'CommentToCodeRatio']\n",
    "\n",
    "X_train.columns = features\n",
    "X_test.columns = features\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "testing_data = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter-based Family\n",
    "\n",
    "Filter-based feature selection techniques search for the best subset of\n",
    "metrics according to an evaluation criterion regardless of model\n",
    "construction. Since constructing models is not required, the use of\n",
    "filter-based feature selection techniques is considered low cost and\n",
    "widely used.\n",
    "\n",
    "**Chi-Squared-based feature selection** {cite}`mchugh2013chi` assesses the\n",
    "importance of metrics with the $\\chi^2$ statistic which is a\n",
    "non-parametric statistical test of independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k features ( k = 3 ) according to Chi-squared statistics are as follows:\n",
      "              Feature         Score\n",
      "1            AddedLOC  26834.488115\n",
      "2       nCoupledClass   1599.996258\n",
      "4  CommentToCodeRatio     64.268658\n"
     ]
    }
   ],
   "source": [
    "# Import for Chi-sqaured-based feature selection technique\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "top_k = 3\n",
    "chi2_fs = SelectKBest(chi2, k=top_k).fit(X_train, y_train)\n",
    "dfscores = pd.DataFrame(chi2_fs.scores_)\n",
    "dfcolumns = pd.DataFrame(features)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Feature','Score'] \n",
    "print('Top-k features ( k =', top_k, ') according to Chi-squared statistics are as follows:')\n",
    "print(featureScores.nlargest(top_k,'Score')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper-based Family\n",
    "\n",
    "\n",
    "Wrapper-based feature selection\n",
    "techniques {cite}`john1994irrelevant`{cite}`kohavi1997wrappers` use classification\n",
    "techniques to assess each subset of metrics and find the best subset of\n",
    "metrics according to an evaluation criterion. Wrapper-based feature\n",
    "selection is made up of three steps, which we described below.\n",
    "\n",
    "*(Step 1) Generate a subset of metrics.* Since it is impossible to\n",
    "evaluate all possible subsets of metrics, wrapper-based feature\n",
    "selection often uses search techniques (e.g., best first, greedy hill\n",
    "climbing) to generate candidate subsets of metrics for evaluation.\n",
    "\n",
    "*(Step 2) Construct a classifier using a subset of metrics with a\n",
    "predetermined classification technique.* Wrapper-based feature selection\n",
    "constructs a classification model using a candidate subset of metrics\n",
    "for a given classification technique (e.g., logistic regression and\n",
    "random forest).\n",
    "\n",
    "*(Step 3) Evaluate the classifier according to a given evaluation\n",
    "criterion.* Once the classifier is constructed, wrapper-based feature\n",
    "selection evaluates the classifier using a given evaluation criterion\n",
    "(e.g., Akaike Information Criterion).\n",
    "\n",
    "For each candidate subset of metrics, wrapper-based feature selection\n",
    "repeats Steps 2 and 3 in order to find the best subset of metrics\n",
    "according to the evaluation criterion. Finally, it provides the best\n",
    "subset of metrics that yields the highest performance according to the\n",
    "evaluation criterion.\n",
    "\n",
    "**Recursive Feature Elimination** (RFE) {cite}`guyon2003introduction`\n",
    "searches for the best subset of metrics by recursively eliminating the\n",
    "least important metric. First, RFE constructs a model using all metrics\n",
    "and ranks metrics according to their importance score (e.g., Breiman's\n",
    "Variable Importance for random forest). In each iteration, RFE excludes\n",
    "the least important metric and reconstructs a model. Finally, RFE\n",
    "provides the subset of metrics which yields the best performance\n",
    "according to an evaluation criterion (e.g., AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k ( k = 3 ) according to the RFE teachnique are as follows:\n",
      "['AddedLOC', 'nCoupledClass', 'CommentToCodeRatio']\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import compress\n",
    "top_k = 3\n",
    "rf_model = RandomForestClassifier(random_state=1234, n_jobs = 10)\n",
    "rfe = RFE(rf_model, n_features_to_select = top_k)\n",
    "rfe_fit = rfe.fit(X_train, y_train)\n",
    "rfe_features = list(compress(features, rfe_fit.support_))\n",
    "print('Top-k (k =', top_k, ') according to the RFE teachnique are as follows:')\n",
    "print(rfe_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Parts of this chapter have been published by Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Christoph Treude:\n",
    "The impact of automated feature selection techniques on the interpretation of defect models. Empir. Softw. Eng. 25(5): 3590-3638 (2020)\"\n",
    "```\n",
    "\n",
    "### Suggested Readings\n",
    "\n",
    "[1] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Christoph Treude:\n",
    "The impact of automated feature selection techniques on the interpretation of defect models. Empir. Softw. Eng. 25(5): 3590-3638 (2020)\n",
    "\n",
    "[2] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Christoph Treude:\n",
    "AutoSpearman: Automatically Mitigating Correlated Software Metrics for Interpreting Defect Models. ICSME 2018: 92-103\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
