{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Explainable AI?\n",
    "\n",
    "Nowadays, Artificial Intelligence\n",
    "\n",
    "AI has entered the business mainstream, opening up opportunities to boost productivity, innovation and fundamentally transform operating models. \n",
    "Yet, as AI becomes more sophisticated, more and more decision making is being performed by an algorithmic **‘black box’**. To have confidence in the outcomes, cement stakeholder trust and ultimately capitalise on the opportunities, it may be necessary to know the rationale of how the algorithm arrived at its recommendation or decision – ‘Explainable AI’. Yet opening up the black box is difficult and may not always be essential. So, when should you lift the lid, and how?\n",
    "\n",
    "\n",
    "```{figure} /xai/images/xai-darpa.png\n",
    "---\n",
    "name: xai-darpa\n",
    "---\n",
    "An illustration of the Explainable AI concept.\n",
    "``` \n",
    "\n",
    "\n",
    "The Explainable AI (XAI) program aims to create a suite of machine learning techniques that:\n",
    "\n",
    "- Produce more explainable models, while maintaining a high level of learning performance (prediction accuracy); and\n",
    "\n",
    "- Enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.\n",
    "\n",
    "\n",
    "\n",
    "Explanations are provided to support transparency,\n",
    "where users can see some aspects of the inner state or\n",
    "functionality of the AI system. When AI is used as a\n",
    "decision aid, users would seek to use explanations to\n",
    "improve their decision making. If the system behaved\n",
    "unexpectedly or erroneously, users would want\n",
    "explanations for scrutability and debugging to be able to\n",
    "identify the offending fault and take control to make\n",
    "corrections. Indeed, this goal is important and has been well\n",
    "studied regarding user models [7, 48] and debugging\n",
    "intelligent agents [59]. Finally, explanations are often\n",
    "proposed to improve trust in the system and specifically\n",
    "moderate trust to an appropriate level [8, 15, 68]. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
