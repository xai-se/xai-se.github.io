{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always evaluate using business-driven measures\n",
    "\n",
    "TODO\n",
    "<!-- \n",
    "First, we use the *Brier score* [@Brier1950; @Rufibach2010] to measure\n",
    "the distance between the predicted probabilities and the outcome. The\n",
    "Brier score is calculated as\n",
    "$\\text{B} = \\frac{1}{N}\\sum\\limits _{i=1}^{N}(f_t-o_t)^2$, where $f_t$\n",
    "is the predicted probability, $o_t$ is the outcome for module $t$\n",
    "encoded as 0 if module $t$ is clean and 1 if it is defective, and $N$ is\n",
    "the total number of modules. The Brier score ranges from 0 (best\n",
    "classifier performance) to 1 (worst classifier performance), where a\n",
    "Brier score of 0.25 is a random-guessing performance.\n",
    "\n",
    "Second, we use the *calibration slope* to measure the direction and\n",
    "spread of the predicted\n",
    "probabilities [@Miller1991; @Jr1996; @Steyerberg2000; @DenBoer2005; @Steyerberg2013; @frank; @Cox1958].\n",
    "The calibration slope is the slope of a logistic regression model that\n",
    "is trained using the predicted probabilities of our original defect\n",
    "prediction model to predict whether a module will be defective or\n",
    "not [@Cox1958]. A calibration slope of 1 indicates the best classifier\n",
    "performance (i.e., the predicted probabilities are consistent with\n",
    "module's labels) and a calibration slope of 0 (or less) indicates the\n",
    "worst classifier performance (i.e., the predicted probabilities are\n",
    "inconsistent with module's labels)\n",
    "\n",
    "Third, we use the *Area Under the receiver operator characteristic Curve\n",
    "(AUC)* to measure the discriminatory power of our models, as suggested\n",
    "by recent research\n",
    "[@Lessmann2008; @frank; @huang2005using; @DenBoer2005; @Steyerberg2008; @Steyerberg2013].\n",
    "The AUC is a threshold-independent performance metric that measures a\n",
    "classifier's ability to discriminate between defective and clean modules\n",
    "(i.e., do the defective modules tend to have higher predicted\n",
    "probabilities than clean modules?). AUC is computed by measuring the\n",
    "area under the curve that plots the true positive rate against the false\n",
    "positive rate, while varying the threshold that is used to determine\n",
    "whether a file is classified as defective or not. Values of AUC range\n",
    "between 0 (worst performance), 0.5 (random guessing performance), and 1\n",
    "(best performance). We use the `val.prob` function of the `rms` R\n",
    "package [@rms] to calculate the Brier score, calibration slope, and AUC.\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
