{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e84c7f",
   "metadata": {},
   "source": [
    "# (Step 5) Model Ranking\n",
    "\n",
    "## Using a Non-Parametric Scott-Knott ESD Test For Multiple Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f6eff",
   "metadata": {},
   "source": [
    "The Non-Parametric ScottKnott ESD (NPSK) test is a multiple comparison approach that leverages a hierarchical clustering to partition the set of median values of techniques (e.g., medians of variable importance scores, medians of model performance) into statistically distinct groups with non-negligible difference. The Non-Parametric ScottKnott ESD (NPSK) does not require the assumptions of normal distributions, homogeneous distributions, and the minimum sample size. The mechanism of the Non-Parametric Scott-Knott ESD test is made up of 2 steps:\n",
    "\n",
    "*(Step 1) Find a partition that maximizes treatment medians between groups.* We begin by sorting the median value of the distributions. Then, we compute the Kruskal Chisq statistics to identify a partition that maximizes the median values between groups. The Kruskal Chisq test is a non-parametric test, which does not require data normality and data heterogeneity assumptions.\n",
    "\n",
    "*(Step 2) Splitting into two groups or merging into one group.* We analyze the magnitude of the difference for each pair for all of the treatment medians of the two groups. If there is any one pair of treatment medians of two groups are non-negligible, we split into two groups. Otherwise, we merge into one group. We use the Cliff $|\\delta|$ effect size to estimate the effect size of the difference between the two medians.\n",
    "\n",
    "To illustrate how the Non-Parametric ScottKnott ESD works in practice, we first prepare a training dataset; construct defect models using 6 classification techniques (i.e., Logistic Regression, Random Forests, C5.0 (Decision Tree), Neural Network, Gradient Boosting Machine, and eXtreme Gradient Boosting Tree); and evaluate them using the AUC measure with the 10-fold cross validation approach.\n",
    "We provide a code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf55a3f6",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:47:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:47:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:47:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:47:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:47:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1635105055642/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klainfo/miniforge3/envs/xaitools/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>RF</th>\n",
       "      <th>DT</th>\n",
       "      <th>NN</th>\n",
       "      <th>GBM</th>\n",
       "      <th>XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.805701</td>\n",
       "      <td>0.575852</td>\n",
       "      <td>0.488532</td>\n",
       "      <td>0.746396</td>\n",
       "      <td>0.662025</td>\n",
       "      <td>0.566186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.830603</td>\n",
       "      <td>0.817824</td>\n",
       "      <td>0.578145</td>\n",
       "      <td>0.677261</td>\n",
       "      <td>0.861402</td>\n",
       "      <td>0.753277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.848296</td>\n",
       "      <td>0.797837</td>\n",
       "      <td>0.596003</td>\n",
       "      <td>0.859764</td>\n",
       "      <td>0.800131</td>\n",
       "      <td>0.776212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.710943</td>\n",
       "      <td>0.617677</td>\n",
       "      <td>0.594949</td>\n",
       "      <td>0.699327</td>\n",
       "      <td>0.728283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.856902</td>\n",
       "      <td>0.864983</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.895960</td>\n",
       "      <td>0.817508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.732323</td>\n",
       "      <td>0.761953</td>\n",
       "      <td>0.510774</td>\n",
       "      <td>0.622559</td>\n",
       "      <td>0.784848</td>\n",
       "      <td>0.711785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.724916</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.772391</td>\n",
       "      <td>0.794949</td>\n",
       "      <td>0.745455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.712795</td>\n",
       "      <td>0.628620</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.622559</td>\n",
       "      <td>0.606902</td>\n",
       "      <td>0.605051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.878016</td>\n",
       "      <td>0.752973</td>\n",
       "      <td>0.569317</td>\n",
       "      <td>0.868841</td>\n",
       "      <td>0.741930</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.646619</td>\n",
       "      <td>0.614849</td>\n",
       "      <td>0.534149</td>\n",
       "      <td>0.671424</td>\n",
       "      <td>0.635406</td>\n",
       "      <td>0.603126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LR        RF        DT        NN       GBM       XGB\n",
       "0  0.805701  0.575852  0.488532  0.746396  0.662025  0.566186\n",
       "1  0.830603  0.817824  0.578145  0.677261  0.861402  0.753277\n",
       "2  0.848296  0.797837  0.596003  0.859764  0.800131  0.776212\n",
       "3  0.677778  0.710943  0.617677  0.594949  0.699327  0.728283\n",
       "4  0.856902  0.864983  0.672222  0.619529  0.895960  0.817508\n",
       "5  0.732323  0.761953  0.510774  0.622559  0.784848  0.711785\n",
       "6  0.724916  0.809091  0.525253  0.772391  0.794949  0.745455\n",
       "7  0.712795  0.628620  0.511111  0.622559  0.606902  0.605051\n",
       "8  0.878016  0.752973  0.569317  0.868841  0.741930  0.629630\n",
       "9  0.646619  0.614849  0.534149  0.671424  0.635406  0.603126"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "matplotlib is required for plotting when the default backend \"matplotlib\" is selected.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/68/p10tkmvx0p9dw88yp1k3tzh00000gr/T/ipykernel_59814/2435358079.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mmodel_performance_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_performance.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_performance_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mmodel_performance_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'box'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AUC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/xaitools/lib/python3.9/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mplot_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_plot_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"backend\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         x, y, kind, kwargs = self._get_call_args(\n",
      "\u001b[0;32m~/miniforge3/envs/xaitools/lib/python3.9/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_get_plot_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_backends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m     \u001b[0m_backends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/xaitools/lib/python3.9/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_load_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas.plotting._matplotlib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m   1755\u001b[0m                 \u001b[0;34m\"matplotlib is required for plotting when the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0;34m'default backend \"matplotlib\" is selected.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected."
     ]
    }
   ],
   "source": [
    "## Load Data and preparing datasets\n",
    "\n",
    "# Import for Load Data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import for Split Data into Training and Testing Samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import for Construct Defect Models (Classification)\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forests\n",
    "from sklearn.tree import DecisionTreeClassifier # C5.0 (Decision Tree)\n",
    "from sklearn.neural_network import MLPClassifier # Neural Network\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting Machine (GBM)\n",
    "import xgboost as xgb # eXtreme Gradient Boosting Tree (xGBTree)\n",
    "\n",
    "# Import for Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_dataset = pd.read_csv((\"../../datasets/lucene-2.9.0.csv\"), index_col = 'File')\n",
    "test_dataset = pd.read_csv((\"../../datasets/lucene-3.0.0.csv\"), index_col = 'File')\n",
    "\n",
    "outcome = 'RealBug'\n",
    "features = ['OWN_COMMIT', 'Added_lines', 'CountClassCoupled', 'AvgLine', 'RatioCommentToCode']\n",
    "\n",
    "# process outcome to 0 and 1\n",
    "train_dataset[outcome] = pd.Categorical(train_dataset[outcome])\n",
    "train_dataset[outcome] = train_dataset[outcome].cat.codes\n",
    "\n",
    "test_dataset[outcome] = pd.Categorical(test_dataset[outcome])\n",
    "test_dataset[outcome] = test_dataset[outcome].cat.codes\n",
    "\n",
    "X_train = train_dataset.loc[:, features]\n",
    "X_test = test_dataset.loc[:, features]\n",
    "\n",
    "y_train = train_dataset.loc[:, outcome]\n",
    "y_test = test_dataset.loc[:, outcome]\n",
    "\n",
    "\n",
    "# commits - # of commits that modify the file of interest\n",
    "# Added lines - # of added lines of code\n",
    "# Count class coupled - # of classes that interact or couple with the class of interest\n",
    "# LOC - # of lines of code\n",
    "# RatioCommentToCode - The ratio of lines of comments to lines of code\n",
    "features = ['nCommit', 'AddedLOC', 'nCoupledClass', 'LOC', 'CommentToCodeRatio']\n",
    "\n",
    "X_train.columns = features\n",
    "X_test.columns = features\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "testing_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "cv_kfold = 10\n",
    "model_performance_df = pd.DataFrame()\n",
    "## Construct defect models and generate the 10-fold Cross Validation AUC\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=1234)\n",
    "model_performance_df['LR'] = cross_val_score(lr_model, X_train, y_train, cv = cv_kfold, scoring = 'roc_auc')\n",
    "\n",
    "# Random Forests\n",
    "rf_model = RandomForestClassifier(random_state=1234, n_jobs = 10)\n",
    "model_performance_df['RF'] = cross_val_score(rf_model, X_train, y_train, cv = cv_kfold, scoring = 'roc_auc')\n",
    "\n",
    "# C5.0 (Decision Tree)\n",
    "dt_model = DecisionTreeClassifier(random_state=1234)\n",
    "model_performance_df['DT'] = cross_val_score(dt_model, X_train, y_train, cv = cv_kfold, scoring = 'roc_auc')\n",
    "\n",
    "# Neural Network\n",
    "nn_model = MLPClassifier(random_state=1234)\n",
    "model_performance_df['NN'] = cross_val_score(nn_model, X_train, y_train, cv = cv_kfold, scoring = 'roc_auc')\n",
    "\n",
    "# Gradient Boosting Machine (GBM)\n",
    "gbm_model = GradientBoostingClassifier(random_state=1234)\n",
    "gbm_model.fit(X_train, y_train)  \n",
    "model_performance_df['GBM'] = cross_val_score(gbm_model, X_train, y_train, cv = cv_kfold, scoring = 'roc_auc')\n",
    "\n",
    "# eXtreme Gradient Boosting Tree (xGBTree)\n",
    "xgb_model = xgb.XGBClassifier(random_state=1234)\n",
    "model_performance_df['XGB'] = cross_val_score(xgb_model, X_train, y_train, cv = cv_kfold, scoring = 'roc_auc')\n",
    "\n",
    "# export to csv, display, and visualise the data frame\n",
    "model_performance_df.to_csv('model_performance.csv', index = False)\n",
    "display(model_performance_df)\n",
    "model_performance_df.plot(kind = 'box', ylim = (0, 1), ylabel = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b600cb",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display(model_performance_df)\n",
    "model_performance_df.plot(kind = 'box', ylim = (0, 1), ylabel = 'AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a692b",
   "metadata": {},
   "source": [
    "*Please note that we neither pre-process data nor optimise the hyper-parameter settings of models in this tutorial. Therefore, the generated performance estimates may be altered if the data is well processed and the hyper-parameter settings are appropriately optimised.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee60f2a",
   "metadata": {},
   "source": [
    "Then, we apply the Non-Parametric ScottKnott ESD approach to the performance estimates of the 6 classification techniques. We use the implementation as provided by the [ScottKnottESD](https://github.com/klainfo/ScottKnottESD) R package.\n",
    "\n",
    "\n",
    "`````{admonition} R code\n",
    ":class: tip\n",
    "````\n",
    "# Import libraries\n",
    "library(ScottKnottESD)\n",
    "library(readr)\n",
    "library(ggplot2)\n",
    "\n",
    "# load data\n",
    "model_performance <- read_csv(\"model_performance.csv\")\n",
    "\n",
    "# apply ScottKnottESD and prepare a ScottKnottESD dataframe\n",
    "sk_results <- sk_esd(model_performance_df)\n",
    "sk_ranks <- data.frame(model = names(sk_results$groups),\n",
    "             rank = paste0('Rank-', sk_results$groups))\n",
    "\n",
    "# prepare a dataframe for generating a visualisation\n",
    "plot_data <- melt(model_performance)\n",
    "plot_data <- merge(plot_data, sk_ranks, by.x = 'variable', by.y = 'model')\n",
    "\n",
    "# generate a visualisation\n",
    "g <- ggplot(data = plot_data, aes(x = variable, y = value, fill = rank)) +\n",
    "     geom_boxplot() +\n",
    "     ylim(c(0, 1)) +\n",
    "     facet_grid(~rank, scales = 'free_x') +\n",
    "     scale_fill_brewer(direction = -1) +\n",
    "     ylab('AUC') + xlab('Model') + ggtitle('') + theme_bw() +\n",
    "     theme(text = element_text(size = 16),\n",
    "           legend.position = 'none')\n",
    "g        \n",
    "````\n",
    "`````\n",
    "\n",
    "\n",
    "\n",
    "```{figure} /defect-prediction/images/scottknott_esd.png\n",
    "---\n",
    "name: scottknott_ranking\n",
    "---\n",
    "A visualisation of model performance ranking according to the Non-Parametric ScottKnott ESD approach.\n",
    "```\n",
    "\n",
    "{numref}`scottknott_ranking` illustrates a visualisation of model performance ranking according to the Non-Parametric ScottKnott ESD approach.\n",
    "According to {numref}`scottknott_ranking`, we find that Logistic Regression appears as the 1st rank; Gradient Boosting Machine and Random Forests appear as the 2nd rank; Neural Network and eXtreme Gradient Boosting Tree appear as the 3rd rank; and, finally, Decision Tree appears as the 4th rank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089ef5d",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Parts of this chapter have been published by Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, Kenichi Matsumoto:\n",
    "An Empirical Comparison of Model Validation Techniques for Defect Prediction Models. IEEE Trans. Software Eng. 43(1): 1-18 (2017).\n",
    "```\n",
    "\n",
    "## Suggested Readings\n",
    "\n",
    "[1] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, Kenichi Matsumoto:\n",
    "An Empirical Comparison of Model Validation Techniques for Defect Prediction Models. IEEE Trans. Software Eng. 43(1): 1-18 (2017)\n",
    "\n",
    "\n",
    "[2] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, Kenichi Matsumoto:\n",
    "The Impact of Automated Parameter Optimization on Defect Prediction Models. IEEE Trans. Software Eng. 45(7): 683-711 (2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2a01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
