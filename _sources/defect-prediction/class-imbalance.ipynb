{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always handle class imbalance\n",
    "\n",
    "### Class Rebalancing Techniques for Software Analytics\n",
    "\n",
    "A plethora of class rebalancing techniques exist {cite}`he2009learning`,\n",
    "e.g., (1) sampling methods for imbalanced learning, (2) cost-sensitive\n",
    "methods for imbalanced learning, (3) kernel-based methods for imbalanced\n",
    "learning, and (4) active learning for imbalanced learning. Since it is\n",
    "impractical to study all of these techniques, we select a manageable set\n",
    "of class rebalancing techniques for our study. As discussed by\n",
    "He {cite}`he2009learning`, we start from the four families of imbalance\n",
    "learning techniques. Based on a literature surveys by Hall *et al.* {cite}`hall2012systematic`,\n",
    "Shihab {cite}`Shihab2012`, and Nam {cite}`nam2014survey`, we then select only the\n",
    "family of sampling techniques for the context of defect prediction.\n",
    "\n",
    "We first select the three commonly-used techniques (i.e., over-sampling,\n",
    "under-sampling, and Default SMOTE {cite}`chawla2002smote`) that were\n",
    "previously used in the literature {cite}`Kamei2007`{cite}`Khoshgoftaar2010`{cite}`Pelayo2007`{cite}`Seiffert2014`{cite}`tan2015online`{cite}`wang2013`{cite}`xia2015elblocker`{cite}`yang2016automated`{cite}`yang2017high`\n",
    "\n",
    "\n",
    "\n",
    "```{figure} /defect-prediction/images/class-imbalance-overview.png\n",
    "---\n",
    "name: class-imbalance-overview\n",
    "---\n",
    "An illustrative overview of the 3 selected class rebalancing techniques.\n",
    "```\n",
    "\n",
    "Below, we provide a description and a discussion of the 3 selected class rebalancing techniques for our book with interactive python tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Load Data and preparing datasets\n",
    "\n",
    "# Import for Load Data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "# Import for Split Data into Training and Testing Samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Import for Construct a black-box model (Regression and Random Forests)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_dataset = pd.read_csv((\"../../datasets/lucene-2.9.0.csv\"), index_col = 'File')\n",
    "test_dataset = pd.read_csv((\"../../datasets/lucene-3.0.0.csv\"), index_col = 'File')\n",
    "\n",
    "outcome = 'RealBug'\n",
    "features = ['OWN_COMMIT', 'Added_lines', 'CountClassCoupled', 'AvgLine', 'RatioCommentToCode']\n",
    "\n",
    "# process outcome to 0 and 1\n",
    "train_dataset[outcome] = pd.Categorical(train_dataset[outcome])\n",
    "train_dataset[outcome] = train_dataset[outcome].cat.codes\n",
    "\n",
    "test_dataset[outcome] = pd.Categorical(test_dataset[outcome])\n",
    "test_dataset[outcome] = test_dataset[outcome].cat.codes\n",
    "\n",
    "X_train = train_dataset.loc[:, features]\n",
    "X_test = test_dataset.loc[:, features]\n",
    "\n",
    "y_train = train_dataset.loc[:, outcome]\n",
    "y_test = test_dataset.loc[:, outcome]\n",
    "\n",
    "\n",
    "# commits - # of commits that modify the file of interest\n",
    "# Added lines - # of added lines of code\n",
    "# Count class coupled - # of classes that interact or couple with the class of interest\n",
    "# LOC - # of lines of code\n",
    "# RatioCommentToCode - The ratio of lines of comments to lines of code\n",
    "features = ['nCommit', 'AddedLOC', 'nCoupledClass', 'LOC', 'CommentToCodeRatio']\n",
    "\n",
    "X_train.columns = features\n",
    "X_test.columns = features\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "testing_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Compute class ratio\n",
    "def get_class_ratio(input_data):\n",
    "    n_data = len(input_data)\n",
    "    n_defective = sum(input_data)\n",
    "    n_clean = n_data - n_defective\n",
    "    print('From the total of', n_data, 'files, there are:')\n",
    "    print(n_defective, 'defective files', '('+str(np.round(n_defective*1.0/n_data*100, 2))+'%)')\n",
    "    print(n_clean, 'clean files', '('+str(np.round(n_clean*1.0/n_data*100, 2))+'%)')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 1368 files, there are:\n",
      "273 defective files (19.96%)\n",
      "1095 clean files (80.04%)\n"
     ]
    }
   ],
   "source": [
    "# Find a class ratio of the original training dataset\n",
    "get_class_ratio(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-Sampling Technique (OVER)\n",
    "\n",
    "The over-sampling technique (a.k.a. up-sampling) randomly samples with\n",
    "replacement (i.e., replicating) *the minority class* (e.g., defective\n",
    "class) to be the same size as the majority class (e.g., clean class).\n",
    "The advantage of an over-sampling technique is that it leads to no\n",
    "information loss. Since oversampling simply adds replicated modules from\n",
    "the original dataset, the disadvantage is that the training dataset ends\n",
    "up with multiple redundant modules, leading to an overfitting. Thus,\n",
    "when applying the over-sampling technique, the performance of with-in\n",
    "defect prediction models is likely higher than the performance of\n",
    "cross-project defect prediction models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 2190 files, there are:\n",
      "1095 defective files (50.0%)\n",
      "1095 clean files (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Import for the Over-Sampling technique (OVER)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Apply the Over-Sampling technique\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_OVER_train, y_OVER_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Find a class ratio of the over-sampled training dataset\n",
    "get_class_ratio(y_OVER_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under-Sampling Technique (UNDER)\n",
    "\n",
    "The under-sampling technique (a.k.a. down-sampling) randomly samples\n",
    "(i.e., reducing) *the majority class* (e.g., clean class) in order to\n",
    "reduce the number of majority modules to be the same number as the\n",
    "minority class (e.g., defective class). The advantage of an\n",
    "under-sampling technique is that it reduces the size of the training\n",
    "data when the original data is relatively large. However, the\n",
    "disadvantage is that removing modules may cause the training data to\n",
    "lose important information pertaining to the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 546 files, there are:\n",
      "273 defective files (50.0%)\n",
      "273 clean files (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Import for the Under-Sampling technique (UNDER)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Apply the Under-Sampling technique\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_UNDER_train, y_UNDER_train = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Find a class ratio of the under-sampled training dataset\n",
    "get_class_ratio(y_UNDER_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "The SMOTE technique {cite}`chawla2002smote` was proposed to combat the\n",
    "disavantages of the simple over-sampling and under-sampling techniques.\n",
    "The SMOTE technique creates artificial data based on the feature space\n",
    "(rather than the data space) similarities from the minority modules. The\n",
    "SMOTE technique starts with a set of minority modules (i.e., defective\n",
    "modules). For each of the minority defective modules of the training\n",
    "datasets, SMOTE performs the following steps:\n",
    "\n",
    "1.  Calculate the $k$-nearest neighbors.\n",
    "\n",
    "2.  Select $N$ majority clean modules based on the smallest magnitude of\n",
    "    the euclidean distances that are obtained from the $k$-nearest\n",
    "    neighbors.\n",
    "\n",
    "Finally, SMOTE combines the synthetic oversampling of the minority\n",
    "defective modules with the undersampling the majority clean modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 2190 files, there are:\n",
      "1095 defective files (50.0%)\n",
      "1095 clean files (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Import for the Synthetic Minority Oversampling Technique (SMOTE)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply the SMOTE technique\n",
    "oversample_SMOTE = SMOTE(sampling_strategy='minority')\n",
    "X_SMOTE_train, y_SMOTE_train = oversample_SMOTE.fit_resample(X_train, y_train)\n",
    "\n",
    "# Find a class ratio of the SMOTE-ed training dataset\n",
    "get_class_ratio(y_SMOTE_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Parts of this chapter have been published by Chakkrit Tantithamthavorn, Ahmed E. Hassan, Kenichi Matsumoto: The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models. IEEE Trans. Software Eng. 46(11): 1200-1219 (2020)\"\n",
    "```\n",
    "\n",
    "\n",
    "### Suggested Readings\n",
    "\n",
    "[1] Chakkrit Tantithamthavorn, Ahmed E. Hassan, Kenichi Matsumoto: The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models. IEEE Trans. Software Eng. 46(11): 1200-1219 (2020)\"\n",
    "\n",
    "[2] Amritanshu Agrawal, Tim Menzies:\n",
    "Is \"better data\" better than \"better data miners\"?: on the benefits of tuning SMOTE for defect prediction. ICSE 2018: 1050-1061\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
